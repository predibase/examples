{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "642d1fb6",
      "metadata": {
        "id": "642d1fb6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/predibase/examples/blob/main/model-augmented-generation/Personalized_Email_Campaigns.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZqQJOLBwGKzK",
      "metadata": {
        "id": "ZqQJOLBwGKzK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install predibase ludwig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e716cd-5d42-4816-a666-279541aca3d1",
      "metadata": {
        "id": "92e716cd-5d42-4816-a666-279541aca3d1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import yaml\n",
        "\n",
        "from predibase import PredibaseClient\n",
        "\n",
        "from ludwig.data.split import get_splitter\n",
        "from ludwig.data.negative_sampling import negative_sample\n",
        "from ludwig.backend.base import LocalBackend\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bfbe747-5965-4748-afd6-e0ea3a1e06d8",
      "metadata": {
        "id": "3bfbe747-5965-4748-afd6-e0ea3a1e06d8"
      },
      "source": [
        "<br/>\n",
        "\n",
        "# Personalized Email Campaigns üì®"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b025b901-6650-4745-9262-cc7e3876ce41",
      "metadata": {
        "id": "b025b901-6650-4745-9262-cc7e3876ce41",
        "tags": []
      },
      "source": [
        "__Recommender System Model --> LLM Generated Personalized Email Campaigns__\n",
        "\n",
        "In this tutorial, we show how to use the Predibase SDK to train a recommender system (recsys) model, then take the results from the recsys model and generate personalized outreach emails via generations from open-source LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65c1dff-6499-4c5c-a637-2faab6fd2c87",
      "metadata": {
        "id": "e65c1dff-6499-4c5c-a637-2faab6fd2c87"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Authentication üîê\n",
        "\n",
        "The first step is to sign into Predibase.\n",
        "\n",
        "- If you do not have a Predibase account set up yet, you may sign up for a free account [here](https://predibase.com/free-trial)\n",
        "- If you already have an account, navigate to Settings -> My Profile and generate a new API token.\n",
        "- Finally, plug in the generated API token in the code below to authenticate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f316e586-a8d2-4b3a-b13f-385971ea0970",
      "metadata": {
        "id": "f316e586-a8d2-4b3a-b13f-385971ea0970"
      },
      "outputs": [],
      "source": [
        "pc = PredibaseClient(\n",
        "    token=\"API TOKEN HERE\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5feb6e-7e11-48a8-9d68-bd7f80c5de59",
      "metadata": {
        "id": "1b5feb6e-7e11-48a8-9d68-bd7f80c5de59",
        "tags": []
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Dataset Preparation üìÑ\n",
        "\n",
        "Next we'll prepare the dataset needed to train the model the recsys model. For this demonstration, we will be using the [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data)  dataset from Kaggle. For this tutorial, we do not need the images, which are the largest part of the download, so we recommend individually downloading `articles.csv`, `customers.csv`, and `transactions_train.csv` to minimize the download process time. This dataset contains the purchase history of customers across time, along with supporting metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VBGeEQuiHkuB",
      "metadata": {
        "cellView": "form",
        "id": "VBGeEQuiHkuB"
      },
      "outputs": [],
      "source": [
        "#@title <h3>Authenticate Kaggle</h3> Navigate to https://www.kaggle.com. Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account) and select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials. Then run this cell to upload kaggle.json to your Colab runtime.\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv \"{fn}\" ~/.kaggle/kaggle.json && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h5vdGuBDIWFV",
      "metadata": {
        "id": "h5vdGuBDIWFV"
      },
      "source": [
        "### Load data\n",
        "Agree to the [Kaggle competition rules](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data) before continuing with the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vw_fv-dcKR0v",
      "metadata": {
        "id": "Vw_fv-dcKR0v"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download --quiet -c h-and-m-personalized-fashion-recommendations -f 'articles.csv'\n",
        "! kaggle competitions download --quiet -c h-and-m-personalized-fashion-recommendations -f 'customers.csv'\n",
        "! kaggle competitions download --quiet -c h-and-m-personalized-fashion-recommendations -f 'transactions_train.csv'\n",
        "! unzip -q -o articles.csv.zip\n",
        "! unzip -q -o customers.csv.zip\n",
        "! unzip -q -o transactions_train.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6694a76c-b7e3-4186-a98a-bac494c97d38",
      "metadata": {
        "id": "6694a76c-b7e3-4186-a98a-bac494c97d38"
      },
      "outputs": [],
      "source": [
        "articles_df = pd.read_csv(\"articles.csv\")\n",
        "customers_df = pd.read_csv(\"customers.csv\")\n",
        "transactions_df = pd.read_csv(\"transactions_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3GMHnnX-L77Y",
      "metadata": {
        "id": "3GMHnnX-L77Y"
      },
      "source": [
        "### Sampling\n",
        "First, we sample the dataset here because of scale. The `transactions_df` has ~31.8 million rows and ~1.3 million unique customers. This is great for training a very strong model, however, processing this data will take considerably longer. For the purposes of this tutorial, we are sampling in the following ways:\n",
        "- Sample from transactions that occured after August 21st, 2020\n",
        "- Sample from customers who purchased 10 or more items after August 21st, 2020\n",
        "- Select 500 customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VCZu6bAjL5ON",
      "metadata": {
        "id": "VCZu6bAjL5ON"
      },
      "outputs": [],
      "source": [
        "# Convert transaction date column to datetime object\n",
        "transactions_df[\"t_dat\"] = pd.to_datetime(transactions_df.t_dat)\n",
        "\n",
        "# Slice dataset to keep everything after \"2020-08-21\"\n",
        "sampled_transactions_df = transactions_df[transactions_df.t_dat > \"2020-08-21\"]\n",
        "del transactions_df\n",
        "\n",
        "# Strip out year and month for splitting/sampling purposes\n",
        "sampled_transactions_df[\"year_month\"] = sampled_transactions_df.t_dat.dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n",
        "\n",
        "# Sample 500 customers with 10 or more transactions, since ~1.3 million total customers takes quite a while to process\n",
        "customer_ids = np.random.choice(sampled_transactions_df.groupby(\"customer_id\").filter(lambda x: len(x) >= 10).customer_id, 500, replace=False)\n",
        "sampled_transactions_df = sampled_transactions_df[sampled_transactions_df.customer_id.isin(customer_ids)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75f44f2-8252-4639-8e00-de0f1bb82b47",
      "metadata": {
        "id": "a75f44f2-8252-4639-8e00-de0f1bb82b47"
      },
      "source": [
        "Next, we're going to merge the three separate dataframes together to form a single transaction dataframe. This dataframe will contain all transactions that took place by the users in the dataset. Because every row is a transaction that took place, we need to add in some examples of transactions that didn't take place - we call these negative samples. We will handle that a little later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a1c26c-a6ab-462d-8e67-e8bf4e879163",
      "metadata": {
        "id": "c7a1c26c-a6ab-462d-8e67-e8bf4e879163"
      },
      "outputs": [],
      "source": [
        "# Merge the transactions and articles dataframes\n",
        "sampled_transactions_df = pd.merge(\n",
        "    sampled_transactions_df,\n",
        "    articles_df,\n",
        "    how=\"left\",\n",
        "    left_on=\"article_id\",\n",
        "    right_on=\"article_id\",\n",
        ")\n",
        "\n",
        "# Merge the transactions and customers dataframes\n",
        "sampled_transactions_df = pd.merge(\n",
        "    sampled_transactions_df,\n",
        "    customers_df,\n",
        "    how=\"left\",\n",
        "    left_on=\"customer_id\",\n",
        "    right_on=\"customer_id\",\n",
        ")\n",
        "\n",
        "# Set label to 1 for all known transactions, since the customer bought the article\n",
        "sampled_transactions_df[\"label\"] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829522ba-447b-40cb-84a0-e4fb3ee66fa1",
      "metadata": {
        "id": "829522ba-447b-40cb-84a0-e4fb3ee66fa1"
      },
      "source": [
        "Now that we have a single dataframe with all transaction, article, and customer data present in every row, we are going to both split and negative sample the dataset.\n",
        "\n",
        "\n",
        "\n",
        "### Split\n",
        "We need to split our data into a training, validation, and test set. In addition to this though, we also need to split in a way that makes sure a given customer's transaction are present in each set. This way, the learning that takes place in the training set can be fairly evaluated in the validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f25b6e-9a96-4a45-8272-68ba4bb50107",
      "metadata": {
        "id": "21f25b6e-9a96-4a45-8272-68ba4bb50107"
      },
      "outputs": [],
      "source": [
        "# Get ludwig datetime splitter to ensure no target leakage by date. Split 70%, 20%, 10% for train, validation, and test sets\n",
        "splitter = get_splitter(\"datetime\", column=\"year_month\", probabilities=(0.7, 0.2, 0.1))\n",
        "\n",
        "# Split per customer_id to ensure that interactions for a customer are across all splits\n",
        "train_dfs, val_dfs, test_dfs = [], [], []\n",
        "for customer_id in sampled_transactions_df[\"customer_id\"].unique():\n",
        "    train_df, val_df, test_df = splitter.split(sampled_transactions_df[sampled_transactions_df[\"customer_id\"] == customer_id], backend=LocalBackend())\n",
        "\n",
        "    train_dfs.append(train_df)\n",
        "    val_dfs.append(val_df)\n",
        "    test_dfs.append(test_df)\n",
        "\n",
        "# Concatenate all customer id specific splits into their respective datasets\n",
        "train_set = pd.concat(train_dfs)\n",
        "val_set = pd.concat(val_dfs)\n",
        "test_set = pd.concat(test_dfs)\n",
        "\n",
        "# Set the split value for each set\n",
        "train_set[\"split\"] = 0\n",
        "val_set[\"split\"] = 1\n",
        "test_set[\"split\"] = 2\n",
        "\n",
        "# Combine train, val, and test set into final dataset\n",
        "full_df = pd.concat([train_set, val_set, test_set])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17797d3-c7d8-48bc-a4bf-128feedaeac7",
      "metadata": {
        "id": "f17797d3-c7d8-48bc-a4bf-128feedaeac7"
      },
      "source": [
        "### Negative sample\n",
        "As mentioned before, we need to add negative samples so that the model can learn to distinguish between something a user would buy, and something a user would not. To that end, we will create some synthetic data points per user by selecting products that we know the user didn't purchase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86debc3-b33d-4772-b857-64b88dcf6060",
      "metadata": {
        "id": "d86debc3-b33d-4772-b857-64b88dcf6060"
      },
      "outputs": [],
      "source": [
        "# Negative sample each split separately\n",
        "train_df = negative_sample(full_df[full_df.split == 0], neg_pos_ratio=10, neg_val=0)\n",
        "val_df = negative_sample(full_df[full_df.split == 1], neg_pos_ratio=10, neg_val=0)\n",
        "test_df = negative_sample(full_df[full_df.split == 2], neg_pos_ratio=10, neg_val=0)\n",
        "\n",
        "# The negative_sample utility from Ludwig only returns a user_id, item_id, and label column. So\n",
        "# we need to define the columns we want to add back into the dataset.\n",
        "article_cols = [\n",
        "    \"prod_name\",\n",
        "    \"product_type_name\",\n",
        "    \"product_group_name\",\n",
        "    \"graphical_appearance_name\",\n",
        "    \"colour_group_name\",\n",
        "    \"perceived_colour_value_name\",\n",
        "    \"perceived_colour_master_name\",\n",
        "    \"department_name\",\n",
        "    \"index_name\",\n",
        "    \"index_group_name\",\n",
        "    \"section_name\",\n",
        "    \"garment_group_name\",\n",
        "    \"detail_desc\",\n",
        "]\n",
        "customer_cols = [\n",
        "    \"customer_id\",\n",
        "    \"FN\",\n",
        "    \"Active\",\n",
        "    \"club_member_status\",\n",
        "    \"fashion_news_frequency\",\n",
        "    \"age\",\n",
        "    \"postal_code\",\n",
        "]\n",
        "\n",
        "# Add back customer and article features defined above\n",
        "articles = full_df[[\"article_id\"] + article_cols].drop_duplicates([\"article_id\"])\n",
        "customers = full_df[customer_cols].drop_duplicates([\"customer_id\"])\n",
        "\n",
        "train_df = pd.merge(train_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
        "train_df = pd.merge(train_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
        "train_df[\"split\"] = 0\n",
        "\n",
        "val_df = pd.merge(val_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
        "val_df = pd.merge(val_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
        "val_df[\"split\"] = 1\n",
        "\n",
        "test_df = pd.merge(test_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
        "test_df = pd.merge(test_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
        "test_df[\"split\"] = 2\n",
        "\n",
        "# Combine train, val, and test sets together to get the final dataset we will train the model with.\n",
        "final_df = pd.concat([train_df, val_df, test_df])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669b1f0e-919a-4f4b-8e30-ffb10eb12360",
      "metadata": {
        "id": "669b1f0e-919a-4f4b-8e30-ffb10eb12360"
      },
      "source": [
        "<br/>\n",
        "\n",
        "__NOTE:__ Because we decided to sample the dataset to a fraction of it's size, the file size of our dataset is under the 1GB limit that Predibase has on file uploads. So for the purposes of this example, we will go straight from a dataframe to a Predibase dataset. However, for production use cases, it is recommended that you use an object storage such as AWS S3 to hold your dataset artifacts. This way you can connect and train on datasets much larger than 1GB in Predibase."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b108ea-85ca-4bb3-bde1-25a1a59addd9",
      "metadata": {
        "id": "12b108ea-85ca-4bb3-bde1-25a1a59addd9"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Connect Data ‚ôæÔ∏è\n",
        "\n",
        "Here we are using the [`create_dataset_from_df`](https://docs.staging.predibase.com/sdk-guide/datasets/dataset_from_df) method which creates a converts a pandas dataframe to a Predibase File Upload Dataset. It is likely that you are using another connection option and should use a different method of creating a Predibase Dataset. We have all the [connection](https://docs.staging.predibase.com/sdk-guide/connections/) and [dataset](https://docs.staging.predibase.com/sdk-guide/datasets/) options available to reference in the [SDK docs](https://docs.staging.predibase.com/sdk-guide/getting-started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb5be08-186f-4bd2-9da3-93251d6e8585",
      "metadata": {
        "id": "1fb5be08-186f-4bd2-9da3-93251d6e8585"
      },
      "outputs": [],
      "source": [
        "# Use the dataset to dataframe SDK method to create a Predibase dataset from our final df above.\n",
        "HM_dataset = pc.create_dataset_from_df(final_df, \"H&M_Recsys_Dataset\")\n",
        "HM_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55683865-5518-46ad-b076-59d2b4869ef3",
      "metadata": {
        "id": "55683865-5518-46ad-b076-59d2b4869ef3"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Engine üöÇ\n",
        "\n",
        "At Predibase, engines are our solution to common compute and infrastructure pain points that everyone runs into while training models. These are problems like:\n",
        "- Encountering Out of Memory errors due to insufficient compute\n",
        "- Challenges distributing a model training job over multiple compute resources\n",
        "- Losing progress when transient issues interrupt the training process\n",
        "\n",
        "Predibase training engines mitigate these issues by:\n",
        "- Analyzing the training job details to assign the right amount of compute\n",
        "- Logic to distribute the training job over the assigned compute resources\n",
        "- Retry logic when things go wrong\n",
        "\n",
        "With this in mind, we will select the engine we want to use for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff04d9f3-db89-40f7-aec9-6b85c995b2a6",
      "metadata": {
        "id": "ff04d9f3-db89-40f7-aec9-6b85c995b2a6"
      },
      "outputs": [],
      "source": [
        "train_engine = pc.get_engine(\"train_engine\")\n",
        "train_engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b51795-764d-4524-83a6-2402728eb4d2",
      "metadata": {
        "id": "30b51795-764d-4524-83a6-2402728eb4d2"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Config üìù\n",
        "\n",
        "Next, we're going to define the config with the specs for our recommender systems model. Here is a readable yaml representation of the config - below I will explain the key parameters we're setting and why.\n",
        "```\n",
        "model_type: ecd\n",
        "input_features:\n",
        "  - name: prod_name\n",
        "    type: text\n",
        "  - name: product_type_name\n",
        "    type: category\n",
        "  - name: product_group_name\n",
        "    type: category\n",
        "  - name: graphical_appearance_name\n",
        "    type: category\n",
        "  - name: colour_group_name\n",
        "    type: category\n",
        "  - name: perceived_colour_value_name\n",
        "    type: category\n",
        "  - name: perceived_colour_master_name\n",
        "    type: category\n",
        "  - name: department_name\n",
        "    type: category\n",
        "  - name: index_name\n",
        "    type: category\n",
        "  - name: index_group_name\n",
        "    type: category\n",
        "  - name: section_name\n",
        "    type: category\n",
        "  - name: garment_group_name\n",
        "    type: category\n",
        "  - name: detail_desc\n",
        "    type: text\n",
        "  - name: customer_id\n",
        "    type: category\n",
        "  - name: FN\n",
        "    type: category\n",
        "  - name: Active\n",
        "    type: category\n",
        "  - name: club_member_status\n",
        "    type: category\n",
        "  - name: fashion_news_frequency\n",
        "    type: category\n",
        "  - name: age\n",
        "    type: number\n",
        "  - name: postal_code\n",
        "    type: category\n",
        "output_features:\n",
        "  - name: label\n",
        "    type: binary\n",
        "    calibration: true\n",
        "preprocessing:\n",
        "  split:\n",
        "    type: fixed\n",
        "combiner:\n",
        "  type: comparator\n",
        "  entity_1:\n",
        "    - prod_name\n",
        "    - product_type_name\n",
        "    - product_group_name\n",
        "    - graphical_appearance_name\n",
        "    - colour_group_name\n",
        "    - perceived_colour_value_name\n",
        "    - perceived_colour_master_name\n",
        "    - department_name\n",
        "    - index_name\n",
        "    - index_group_name\n",
        "    - section_name\n",
        "    - garment_group_name\n",
        "    - detail_desc\n",
        "  entity_2:\n",
        "    - customer_id\n",
        "    - FN\n",
        "    - Active\n",
        "    - club_member_status\n",
        "    - fashion_news_frequency\n",
        "    - age\n",
        "    - postal_code\n",
        "trainer:\n",
        "  batch_size: 1024\n",
        "```\n",
        "A few of the key parameters set are outlined below:\n",
        "- `output_features.calibration`:\n",
        "- `preprocessing.split.type`:\n",
        "- `combiner.type`:\n",
        "- `combiner.entity_1`:\n",
        "- `combiner.entity_2`:\n",
        "\n",
        "For more configuration details, check out the [Ludwig LLM Docs](https://ludwig.ai/0.8/configuration/large_language_model/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eebcabd4-c7b9-42f5-bbca-96523e839437",
      "metadata": {
        "id": "eebcabd4-c7b9-42f5-bbca-96523e839437"
      },
      "outputs": [],
      "source": [
        "recsys_config = {\n",
        "    'model_type': 'ecd',\n",
        "    'input_features': [\n",
        "        {'name': 'prod_name', 'type': 'text'},\n",
        "        {'name': 'product_type_name', 'type': 'category'},\n",
        "        {'name': 'product_group_name', 'type': 'category'},\n",
        "        {'name': 'graphical_appearance_name', 'type': 'category'},\n",
        "        {'name': 'colour_group_name', 'type': 'category'},\n",
        "        {'name': 'perceived_colour_value_name', 'type': 'category'},\n",
        "        {'name': 'perceived_colour_master_name', 'type': 'category'},\n",
        "        {'name': 'department_name', 'type': 'category'},\n",
        "        {'name': 'index_name', 'type': 'category'},\n",
        "        {'name': 'index_group_name', 'type': 'category'},\n",
        "        {'name': 'section_name', 'type': 'category'},\n",
        "        {'name': 'garment_group_name', 'type': 'category'},\n",
        "        {'name': 'detail_desc', 'type': 'text'},\n",
        "        {'name': 'customer_id', 'type': 'category'},\n",
        "        {'name': 'FN', 'type': 'category'},\n",
        "        {'name': 'Active', 'type': 'category'},\n",
        "        {'name': 'club_member_status', 'type': 'category'},\n",
        "        {'name': 'fashion_news_frequency', 'type': 'category'},\n",
        "        {'name': 'age', 'type': 'number'},\n",
        "        {'name': 'postal_code', 'type': 'category'}\n",
        "    ],\n",
        "    'output_features': [\n",
        "        {'name': 'label', 'type': 'binary', 'calibration': True}\n",
        "    ],\n",
        "    'preprocessing': {\n",
        "        'split': {\n",
        "            'type': 'fixed'\n",
        "        }\n",
        "    },\n",
        "    'combiner': {\n",
        "        'type': 'comparator',\n",
        "        'entity_1': [\n",
        "            'prod_name',\n",
        "            'product_type_name',\n",
        "            'product_group_name',\n",
        "            'graphical_appearance_name',\n",
        "            'colour_group_name',\n",
        "            'perceived_colour_value_name',\n",
        "            'perceived_colour_master_name',\n",
        "            'department_name',\n",
        "            'index_name',\n",
        "            'index_group_name',\n",
        "            'section_name',\n",
        "            'garment_group_name',\n",
        "            'detail_desc'\n",
        "        ],\n",
        "        'entity_2': [\n",
        "            'customer_id',\n",
        "            'FN',\n",
        "            'Active',\n",
        "            'club_member_status',\n",
        "            'fashion_news_frequency',\n",
        "            'age',\n",
        "            'postal_code'\n",
        "        ]\n",
        "    },\n",
        "    'trainer': {\n",
        "        'batch_size': 1024,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f01e677-2692-4a7a-9236-c79368a3d09c",
      "metadata": {
        "id": "4f01e677-2692-4a7a-9236-c79368a3d09c"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Model Training üèÅ\n",
        "\n",
        "Finally, we can kick off our model training job! With this call, we will create both a [Model Repository](https://docs.predibase.com/user-guide/models/model-repos) and train our first recsys model in that repo. As you can see, all of the pieces above have been plugged into this function call. Once you run this cell, you can click on the link to track the fine-tuning progress in the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f7efff-5372-4937-9e73-403b3e35124c",
      "metadata": {
        "id": "d8f7efff-5372-4937-9e73-403b3e35124c"
      },
      "outputs": [],
      "source": [
        "HM_recsys_model = pc.create_model(\n",
        "    repository_name=\"H&M Recommender System Model\",\n",
        "    dataset=HM_dataset,\n",
        "    config=recsys_config,\n",
        "    engine=train_engine,\n",
        "    repo_description=\"Recommend fashion products to customers\",\n",
        "    model_description=\"Baseline Model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cff22a-0a8a-4ddd-8611-4e8cf1cf188c",
      "metadata": {
        "id": "38cff22a-0a8a-4ddd-8611-4e8cf1cf188c"
      },
      "source": [
        "## Deployment and Prediction üéØ\n",
        "\n",
        "Now that our model has finished training, we can deploy and start generating predictions. There are a few ways that you can generate predictions from a Predibase model:\n",
        "\n",
        "1. __REST API__ - The [deployment object](https://docs.predibase.com/user-guide/supervised-ml/deployments/) has an attribute called *deployment_url* which you can make an HTTP  request to in order to get predictions.\n",
        "\n",
        "2. __deployment.predict()__ - You can call `predict()` on the deployment object itself, passing in a dataframe. Just make sure to install the predibase predictor with `pip install \"predibase[predictor]\"`.\n",
        "\n",
        "3. __PQL__ - You can predict directly on data within Predibase using Predictive Query Language (PQL), our extension to SQL that allows you to run predictions with models trained in Predibase on data selected with SQL.\n",
        "\n",
        "Our deployment API is recommended for real-time inference whereas PQL is tailored to batch prediction. For this example though, we will use PQL which is available in the free trial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa8866a-dc15-4f14-95d0-91bab7778f19",
      "metadata": {
        "id": "5aa8866a-dc15-4f14-95d0-91bab7778f19"
      },
      "source": [
        "<br/>\n",
        "\n",
        "For these prediction examples, we're going to grab a random customer from the test set and generate recomendations with the products that they've interacted with in this dataset.\n",
        "\n",
        "Generally speaking, candidate products to generate recommendations with are generated with another service (collaborative filtering, content-based filtering, etc.). The output of this service becomes the input to the recsys model that we've built, which will rank the provided candidates, letting us know which ones to recommend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d7a466-2b8d-4a2f-87c7-6d469dffea6d",
      "metadata": {
        "id": "d5d7a466-2b8d-4a2f-87c7-6d469dffea6d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Select random customer to generate predictions for\n",
        "random_customer_id = test_df.customer_id.sample(1).iloc[0]\n",
        "\n",
        "# Grab the set of products that this customer has interacted with - the input to our model\n",
        "prediction_data = test_df[test_df.customer_id == random_customer_id]\n",
        "\n",
        "# Run inference using PQL. This might take a while the first time.\n",
        "# In a production setting, you'd use the deployment API for real-time predictions.\n",
        "preds = HM_recsys_model.predict('label', source=prediction_data, probabilities=True)\n",
        "\n",
        "# Convert output vector into propensity to purchase value\n",
        "preds.label_probabilities = preds.label_probabilities.apply(lambda x: x[1])\n",
        "\n",
        "# Add propensity to purchase to product data\n",
        "prediction_data = prediction_data.join(preds[[\"label_probabilities\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c190f944-d18c-417a-942c-3087980ae350",
      "metadata": {
        "id": "c190f944-d18c-417a-942c-3087980ae350"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Generate Targeted Emails ü¶ô\n",
        "\n",
        "At last, we can use the recommended products to generate targeted emails for the customer based on the customer + product info. We will be using Predibase LLM capabilities to prompt LLaMa2-7B and generate these custom tailored emails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e6d86f-65d5-401f-b4cf-a70dc03f3515",
      "metadata": {
        "id": "e1e6d86f-65d5-401f-b4cf-a70dc03f3515"
      },
      "outputs": [],
      "source": [
        "def generate_custom_email(product: pd.Series):\n",
        "    \"\"\"\n",
        "    Function that takes in a row of a pandas dataframe containing product and user information\n",
        "    and returns a custom tailored email advertising that product.\n",
        "\n",
        "    :param product: A pandas Series containing product details\n",
        "    :return: Custom generated email\n",
        "    \"\"\"\n",
        "    # Extract and format product information\n",
        "    product_info = \", \".join(f\"{key}: {val}\" for key, val in product[article_cols].items())\n",
        "\n",
        "    # Construct prompt containing product info to feed into LLM\n",
        "    prompt = f\"\"\"\n",
        "    Generate a personalized email for an outreach campaign advertising a product.\n",
        "    Product information: '{product_info}'.\n",
        "    Email:\n",
        "    \"\"\"\n",
        "\n",
        "    return pc.prompt(prompt, \"llama-2-13b-chat\", options={\"max_new_tokens\": 512}).response[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2523cdb3-3c4a-4dc7-a806-70583b736fcb",
      "metadata": {
        "id": "2523cdb3-3c4a-4dc7-a806-70583b736fcb"
      },
      "outputs": [],
      "source": [
        "# Grab the top three recommendations to generate custom emails for\n",
        "top_recommendations = prediction_data.sort_values(\"label_probabilities\", ascending=False).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4964084-5e7f-4676-aaac-45eb6ef5797c",
      "metadata": {
        "id": "e4964084-5e7f-4676-aaac-45eb6ef5797c"
      },
      "source": [
        "<br/>\n",
        "\n",
        "### Email to be sent out on campaign round 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31228123-4165-425c-9682-ab86f394298a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31228123-4165-425c-9682-ab86f394298a",
        "outputId": "a59b535b-a352-4fda-a2cf-717117f217c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Subject: üå∏ Introducing the Angie Blouse - Elevate Your Wardrobe with Timeless Elegance üå∏\n",
            "    \n",
            "    Dear [Recipient's Name],\n",
            "    \n",
            "    We hope this email finds you well! üòä\n",
            "    \n",
            "    We are thrilled to introduce our latest addition to our Ladieswear collection - the stunning Angie Blouse! üíÉ\n",
            "    \n",
            "    This beautiful blouse is crafted from a high-quality textured weave with a subtle sheen, giving it a sophisticated and timeless look. The concealed buttons at the top and small stand-up collar with a frill add a touch of elegance, while the double-layered yoke and gathered seam at the back create a flattering fit. The long, voluminous sleeves with wide, buttoned cuffs complete the look, making it perfect for any formal or semi-formal occasion. üë©‚Äçüíºüë©‚Äçüé§\n",
            "    \n",
            "    The Angie Blouse is part of our Garment Upper body category, and we have carefully selected the Off White colour with a Dusty Light perceived colour value to ensure it complements a variety of skin tones and outfits. üå∏üë†\n",
            "    \n",
            "    Don't miss out on the opportunity to elevate your wardrobe with this timeless piece! Shop the Angie Blouse now and experience the perfect blend of style, comfort, and quality. üíï\n",
            "    \n",
            "    Click the link below to shop now:\n",
            "    [Insert link]\n",
            "    \n",
            "    We hope you enjoy wearing the Angie Blouse as much as we enjoyed creating it! üíï\n",
            "    \n",
            "    Best regards,\n",
            "    [Your Name]\n",
            "    [Your Company]\n",
            "    \n",
            "    P.S. Don't forget to follow us on social media to stay up-to-date with our latest collections and promotions! üíï\n",
            "    [Insert social media links]\n",
            "    \n",
            "    Thank you for considering our product! We appreciate your time and look forward to serving you. üíï\n"
          ]
        }
      ],
      "source": [
        "email_1 = generate_custom_email(top_recommendations.iloc[0])\n",
        "print(email_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a04e53-896c-49a5-ab27-a006b703ae9f",
      "metadata": {
        "id": "b3a04e53-896c-49a5-ab27-a006b703ae9f"
      },
      "source": [
        "<br/>\n",
        "\n",
        "### Email to be sent out on campaign round 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75a57c7-eaa1-42fb-8858-74557c7946da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f75a57c7-eaa1-42fb-8858-74557c7946da",
        "outputId": "f19220b8-f932-45cb-c209-7eb01e2a9f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Subject: üíÉüèª Shimmering Socks for Your Nighttime Adventures! üíÉüèª\n",
            "    \n",
            "    Hey [Name],\n",
            "    \n",
            "    Are you tired of boring socks ruining your nighttime adventures? üò¥ Well, we've got just the thing for you! üòç\n",
            "    \n",
            "    Introducing our new line of 1p Lurex Socks - the perfect accessory for any night out or cozy night in! üíÉüèª These socks are made of a soft, rib-knit cotton blend containing glittery threads that will add a touch of sparkle to your outfit. üíé\n",
            "    \n",
            "    Not only do they look amazing, but they're also super comfortable and perfect for any occasion. Whether you're going out with friends, having a movie night, or just lounging around the house, these socks have got you covered! üòä\n",
            "    \n",
            "    So why wait? ü§î Treat yourself to a pair (or two, or three... üòâ) today and add a little bit of glamour to your nighttime routine. Plus, with our affordable prices, you can stock up and never sacrifice style for comfort again! üí∞\n",
            "    \n",
            "    Click the link below to shop now and get ready to shimmer and shine! üíÉüèª\n",
            "    \n",
            "    [Insert CTA button or link here]\n",
            "    \n",
            "    Happy shopping, [Name]! üíñ\n",
            "    \n",
            "    Best,\n",
            "    [Your Name]\n",
            "    \n",
            "    P.S. Don't forget to follow us on social media for more fashionable finds and exclusive deals! üíï\n",
            "    \n",
            "    [Your Social Media Links]\n",
            "    \n",
            "    Hope this helps! Let me know if you need any further assistance.\n"
          ]
        }
      ],
      "source": [
        "email_2 = generate_custom_email(top_recommendations.iloc[1])\n",
        "print(email_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf11275-ebb2-4701-9657-874c3dfddc37",
      "metadata": {
        "id": "ebf11275-ebb2-4701-9657-874c3dfddc37"
      },
      "source": [
        "<br/>\n",
        "\n",
        "### Email to be sent out on campaign round 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05107c72-ca57-4dc3-9a72-4967c094bc5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05107c72-ca57-4dc3-9a72-4967c094bc5d",
        "outputId": "ef6fa643-6d73-4802-f21f-eb6b80ade8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Subject: üî• Get Ready to Elevate Your Style with Our Latest Addition - Siri Basic Cardigan üî•\n",
            "    \n",
            "    Dear [Recipient Name],\n",
            "    \n",
            "    We hope this email finds you well! üòä\n",
            "    \n",
            "    We are thrilled to introduce our latest addition to our Divided Collection - the stunning Siri Basic Cardigan! üòç This cardigan is a must-have for any fashion-forward individual looking to elevate their style this season.\n",
            "    \n",
            "    Here are some key features of the Siri Basic Cardigan:\n",
            "    \n",
            "    ‚Ä¢ Soft knit with dropped shoulders, long sleeves, front pockets and no buttons\n",
            "    ‚Ä¢ Perfect for both casual and dressy occasions\n",
            "    ‚Ä¢ Available in Dark Pink, a versatile and timeless colour that will complement any outfit\n",
            "    ‚Ä¢ Part of our Tops Knitwear department, ensuring a comfortable and stylish fit\n",
            "    \n",
            "    We believe that this cardigan is a game-changer for anyone looking to add a touch of sophistication and elegance to their wardrobe. With its dropped shoulders, long sleeves, and front pockets, it's perfect for both casual and dressy occasions. Plus, the soft knit material ensures all-day comfort. üòå\n",
            "    \n",
            "    Don't miss out on the opportunity to elevate your style with the Siri Basic Cardigan! Shop now and experience the comfort and sophistication that our Divided Collection has to offer. üíÉ\n",
            "    \n",
            "    Click the link below to shop now: [Insert Link]\n",
            "    \n",
            "    We hope you enjoy shopping with us! üíñ\n",
            "    \n",
            "    Best regards,\n",
            "    [Your Name]\n",
            "    \n",
            "    P.S. Don't forget to follow us on social media to stay up-to-date on the latest fashion trends and promotions! üíï\n",
            "    \n",
            "    [Your Social Media Links]\n",
            "    \n",
            "    This email is sent to [Recipient Name] as part of an outreach campaign to promote the Siri Basic Cardigan. The email aims to highlight the key features of the product and entice the recipient to shop the product by providing a\n"
          ]
        }
      ],
      "source": [
        "email_3 = generate_custom_email(top_recommendations.iloc[2])\n",
        "print(email_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3be6c9-4dbe-45c4-88d0-e26c1b6eb5ab",
      "metadata": {
        "id": "7b3be6c9-4dbe-45c4-88d0-e26c1b6eb5ab"
      },
      "source": [
        "<br/>\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "And there you have it, an end to end tutorial on how to set up a recommender system model in Predibase, and chain the outputs of this model with Predibase's LLM capabilities to generate custom tailored emails for the recommended products.\n",
        "\n",
        "When it comes to the generation step, there is actually a lot of control you have over the generated email via the prompt you pass in. As you saw in the `generate_custom_email` function, we were using a pretty generic prompt where we just pass in the product info and ask the LLM to generate an advertising email. However, you can try all sorts of things with the prompt such as adjusting, tone, length, or even passing in user information to put into the email if you have it available.\n",
        "\n",
        "The purpose of this tutorial was to provide a basic guideline for building solutions of this nature. In addition to this tutorial notebook, we also have a [sample application](https://github.com/predibase/examples/tree/main/model-augmented-generation) that we've generated to showcase what a simple application built with this type of tooling could look like. We hope you enjoyed this tutorial!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "predibase38",
      "language": "python",
      "name": "predibase38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
